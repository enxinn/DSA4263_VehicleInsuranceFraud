{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea224e0-074a-47c4-8dfd-7b2509933311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a66a0e-c26c-4521-89b9-fc580e9a6359",
   "metadata": {},
   "source": [
    "### Reading in data and splitting into train, validation, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28309c5b-b4ac-40ed-9737-73296e1e1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fraud.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d079d4af-bd36-4851-b4fe-8d2bdf64e51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15420, 45)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91aca8b-b838-48ca-90e1-ddb4b7ae0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and response\n",
    "X = df.drop(columns=['FraudFound_P'])\n",
    "y = df['FraudFound_P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df3f01d2-0e99-435a-b8d8-0bffbd62d381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (12336, 44) (12336,)\n",
      "Testing set shape: (3084, 44) (3084,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split into training (80% of whole dataset) and testing (20% of whole dataset) \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,   # ensures reproducibility\n",
    "    stratify=y         # preserves class balance\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce10b1ef-b3c9-4f5e-bb03-2d5773349634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (9252, 44) (9252,)\n",
      "Validation set shape: (3084, 44) (3084,)\n",
      "Testing set shape: (3084, 44) (3084,)\n"
     ]
    }
   ],
   "source": [
    "# Further split training into training (60% of whole dataset) and validation (20% of whole dataset) \n",
    "X_subtrain, X_val, y_subtrain, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.25,\n",
    "    random_state=42,   # ensures reproducibility\n",
    "    stratify=y_train         # preserves class balance\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_subtrain.shape, y_subtrain.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448838c-bb58-4ab9-b7d3-4ff0fc16eb5f",
   "metadata": {},
   "source": [
    "Above, we have effectively split the data into 60% train, 20% validation, and 20% test. The training and validation sets will be used for the train-validation set approach, while the full training set (train and validation combined) will be used to perform cross-validation later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8317f-f821-4075-9e1f-1db627d1eecc",
   "metadata": {},
   "source": [
    "### Function to check class proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "429c4f41-2351-4c90-bf08-f815019a7189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_proportions(y):\n",
    "    \"\"\"\n",
    "    Prints the count and proportion of each class in a pandas Series or array.\n",
    "    \"\"\"\n",
    "    y_series = pd.Series(y, name=\"label\")\n",
    "    counts = y_series.value_counts().sort_index()\n",
    "    proportions = y_series.value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        \"count\": counts,\n",
    "        \"proportion\": proportions.round(4)\n",
    "    })\n",
    "    \n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6c3dc-1244-439f-a78b-b60b7f637bc9",
   "metadata": {},
   "source": [
    "## Logistic Regression (Base Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fedbd59-4927-4d35-9ba5-f4595a4f853f",
   "metadata": {},
   "source": [
    "### Train-validation set approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e693830-f599-4b22-9a7b-009a877d2a43",
   "metadata": {},
   "source": [
    "Now, we use the train-validation set approach to conduct hyperparameter tuning on the logistic regression model. The model with the combination of hyperparameters which produces the highest validation F1-score will be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dc8de05-25b8-41f6-a881-d0e53b2dd7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train–Validation Hyperparameter Tuning Results ===\n",
      "Best Parameters: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear', 'class_weight': 'balanced'}\n",
      "Best F1 Score (Validation): 0.2241\n",
      "\n",
      "=== Metrics for Selected Model (Optimal Hyperparameters) ===\n",
      "Accuracy  : 0.6767\n",
      "Precision : 0.1308\n",
      "Recall    : 0.7826\n",
      "F1        : 0.2241\n",
      "ROC-AUC   : 0.7831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "best_f1 = 0\n",
    "best_params = None\n",
    "best_metrics = {}\n",
    "\n",
    "# --- Hyperparameter grid ---\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear'],      # supports both L1 and L2\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# --- Loop through all combinations ---\n",
    "for C, penalty, solver, class_weight in product(\n",
    "    param_grid['C'],\n",
    "    param_grid['penalty'],\n",
    "    param_grid['solver'],\n",
    "    param_grid['class_weight']\n",
    "):\n",
    "    try:\n",
    "        # Train model\n",
    "        model = LogisticRegression(\n",
    "            C=C,\n",
    "            penalty=penalty,\n",
    "            solver=solver,\n",
    "            class_weight=class_weight,\n",
    "            max_iter=1000,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_subtrain, y_subtrain)\n",
    "\n",
    "        # Predict on validation set\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_val_prob = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_val, y_val_pred)\n",
    "        prec = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "        rec = recall_score(y_val, y_val_pred, zero_division=0)\n",
    "        f1 = f1_score(y_val, y_val_pred, zero_division=0)\n",
    "        roc_auc = roc_auc_score(y_val, y_val_prob)\n",
    "\n",
    "        # Track the best model (based on F1)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_params = {\n",
    "                'C': C,\n",
    "                'penalty': penalty,\n",
    "                'solver': solver,\n",
    "                'class_weight': class_weight\n",
    "            }\n",
    "            best_metrics = {\n",
    "                'Accuracy': acc,\n",
    "                'Precision': prec,\n",
    "                'Recall': rec,\n",
    "                'F1': f1,\n",
    "                'ROC-AUC': roc_auc\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped combination (C={C}, penalty={penalty}) due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n=== Train–Validation Hyperparameter Tuning Results ===\")\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(f\"Best F1 Score (Validation): {best_f1:.4f}\\n\")\n",
    "\n",
    "print(\"=== Metrics for Selected Model (Optimal Hyperparameters) ===\")\n",
    "for metric, value in best_metrics.items():\n",
    "    print(f\"{metric:10s}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3981a-9f70-4c9b-a7a7-c832db8bcb83",
   "metadata": {},
   "source": [
    "The results of the train-validation approach suggest that the model with the hyperparameter combination: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear', 'class_weight': 'balanced'} is the optimal one which produces the highest validation F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4478f28-b5e4-402d-bab0-854adc2164a3",
   "metadata": {},
   "source": [
    "### 5-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a025f1a-7868-4b52-9fa6-a57ed47f2c85",
   "metadata": {},
   "source": [
    "Here, 5-fold CV is performed on the full training set to conduct hyperparameter tuning. We keep the actual test set separate, reserving it for the final model evaluation. The model with the combination of hyperparameters which yields the highest average cross-validated F1-score will be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f960e1e-4ea7-4cce-b9d7-0b0eb30c1eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "=== Model with Optimal Hyperparameters from Grid Search ===\n",
      "Optimal Parameters: {'C': 0.1, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best F1 Score (CV mean): 0.2158\n",
      "\n",
      "=== 5-Fold Cross-Validation (Logistic Regression with Optimal Hyperparameters) ===\n",
      "accuracy  : 0.6579 ± 0.0104\n",
      "precision : 0.1251 ± 0.0036\n",
      "recall    : 0.7877 ± 0.0542\n",
      "f1        : 0.2158 ± 0.0073\n",
      "roc_auc   : 0.7826 ± 0.0149\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "\n",
    "# Logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Hyperparameter grid \n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear'],      # solvers that support l1/l2 regularization\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# --- Scoring metrics ---\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'roc_auc': make_scorer(roc_auc_score, response_method='predict_proba')\n",
    "}\n",
    "\n",
    "# --- Stratified 5-fold cross-validation setup ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- Grid search (optimize for F1 score) ---\n",
    "grid = GridSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    error_score='raise'  # show which combos fail\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"\\n=== Model with Optimal Hyperparameters from Grid Search ===\")\n",
    "print(\"Optimal Parameters:\", grid.best_params_)\n",
    "print(f\"Best F1 Score (CV mean): {grid.best_score_:.4f}\")\n",
    "best_model = grid.best_estimator_\n",
    "cv_results = cross_validate(best_model, X_train, y_train, cv=cv, scoring=scoring)\n",
    "\n",
    "print(\"\\n=== 5-Fold Cross-Validation (Logistic Regression with Optimal Hyperparameters) ===\")\n",
    "for metric in scoring.keys():\n",
    "    print(f\"{metric:10s}: {cv_results[f'test_{metric}'].mean():.4f} ± {cv_results[f'test_{metric}'].std():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a018d2a9-84c7-4df4-81ba-bd6abb5f9af3",
   "metadata": {},
   "source": [
    "The 5-fold CV results suggest that the model with the hyperparameter combination: {'C': 0.1, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'liblinear'} is the optimal one which produces the highest average cross-validated F1-score. The optimal hyperparameter combination selected by 5-fold CV is similar to that chosen using the train-validation approach earlier. As 5-fold CV provides a more robust performance estimate by averaging across multiple folds, we will proceed with using this model chosen by 5-fold CV as our representative logistic regression model to compare against other candidate models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
